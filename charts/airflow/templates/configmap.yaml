################################
## Airflow ConfigMap
#################################
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "airflow_config" . }}
  labels:
    tier: airflow
    component: config
    release: {{ .Release.Name }}
    platform: {{ .Values.platform.release }}
    workspace: {{ .Values.platform.workspace | quote }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    heritage: {{ .Release.Service }}
data:
  # These are system-specified config overrides.
  airflow.cfg: |
    [core]
    load_examples = False
    colored_console_log = False
    executor = {{ .Values.executor }}
  {{- if .Values.elasticsearch.enabled }}
    remote_logging = True
  {{- end }}

  {{- if eq .Values.executor "KnativeExecutor" }}
    [knative]
    knative_host = {{ .Release.Name }}-knative.{{ .Release.Namespace }}.svc.cluster.local
  {{- end }}


  [webserver]
    base_url = {{ printf "https://%s" (include "airflow_url" .) }}
    expose_config = True
    rbac = True

    [celery]
    default_queue = celery

    [scheduler]
    scheduler_heartbeat_sec = 5
    statsd_on = True
    statsd_port = 9125
    statsd_prefix = airflow
    statsd_host = {{ printf "%s-statsd" .Release.Name }}
    # Restart Scheduler every 41460 seconds (11 hours 31 minutes)
    # The odd time is chosen so it is not always restarting on the same "hour" boundary
    run_duration = 41460

    [elasticsearch]
    # The elasticsearch variables were updated to the shorter names in v1.10.4
    write_stdout = True
    json_format = True
    log_id_template = {dag_id}_{task_id}_{execution_date}_{try_number}

    # This is the older format for these variable names, kept here for backward compatibility
    elasticsearch_write_stdout = True
    elasticsearch_json_format = True
    elasticsearch_log_id_template = {dag_id}_{task_id}_{execution_date}_{try_number}

    [kubernetes]
    namespace = {{ .Release.Namespace }}
    airflow_configmap = {{ include "airflow_config" . }}
    worker_container_repository = {{ .Values.images.airflow.repository }}
    worker_container_tag = {{ .Values.images.airflow.tag }}
    worker_container_image_pull_policy = {{ .Values.images.airflow.pullPolicy }}
    worker_service_account_name = {{ .Release.Name }}-worker-serviceaccount
    image_pull_secrets = {{ template "registry_secret" . }}
    dags_in_image = True
    delete_worker_pods = True

    [kubernetes_secrets]
    AIRFLOW__CORE__SQL_ALCHEMY_CONN = {{ printf "%s=connection" (include "airflow_metadata_secret" .) }}
    AIRFLOW__CORE__FERNET_KEY = {{ printf "%s=fernet-key" (include "fernet_key_secret" .) }}
    AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_HOST = {{ printf "%s=connection" (include "elasticsearch_secret" .) }}

    [kubernetes_labels]
    tier = airflow
    component = worker
    release = {{ .Release.Name }}
    platform = {{ .Values.platform.release }}
    workspace = {{ .Values.platform.workspace }}

    [astronomer]
    jwt_signing_cert = {{ template "airflow_webserver_jwt_cert_path" . }}
    jwt_audience = {{ include "airflow_url" . }}

  webserver_config.py: |
    import os
    from airflow import configuration as conf
    from flask_appbuilder.security.manager import AUTH_REMOTE_USER
    basedir = os.path.abspath(os.path.dirname(__file__))

    # The SQLAlchemy connection string.
    SQLALCHEMY_DATABASE_URI = conf.get('core', 'SQL_ALCHEMY_CONN')

    # Flask-WTF flag for CSRF
    CSRF_ENABLED = True

    # ----------------------------------------------------
    # AUTHENTICATION CONFIG
    # ----------------------------------------------------
    AUTH_TYPE = AUTH_REMOTE_USER

    from astronomer.flask_appbuilder.security import AirflowAstroSecurityManager
    SECURITY_MANAGER_CLASS = AirflowAstroSecurityManager

  airflow_local_settings.py: |
    from airflow.contrib.kubernetes.pod import Pod
    from airflow.configuration import conf


    def pod_mutation_hook(pod: Pod):

        extra_labels = {
            "kubernetes-executor": "False",
            "kubernetes-pod-operator": "False"
        }

        if 'airflow-worker' in pod.labels.keys() or \
                conf.get('core', 'EXECUTOR') == "KubernetesExecutor":
            extra_labels["kubernetes-executor"] = "True"
        else:
            extra_labels["kubernetes-pod-operator"] = "True"

        pod.labels.update(extra_labels)
