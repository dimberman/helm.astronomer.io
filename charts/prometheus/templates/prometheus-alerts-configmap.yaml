################################
## Prometheus Alerts ConfigMap
#################################
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ template "prometheus.fullname" . }}-alerts
  labels:
    tier: monitoring
    component: {{ template "prometheus.name" . }}
    chart: {{ template "prometheus.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  alerts.yaml: |-
    groups:
{{ toYaml .Values.alerts.groups | indent 6 }}
      - name: platform
        rules:
        - alert: PrometheusDiskUsage
          expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-{{ template "prometheus.fullname" . }}-.*"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-{{ template "prometheus.fullname" . }}-.*"} * 100) < 10
          for: 5m
          labels:
            tier: platform
            component: prometheus
          annotations:
            summary: "Prometheus High Disk Usage"
            description: "Prometheus has less than 10% disk space available."

        - alert: RegistryDiskUsage
          expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-{{ .Release.Name }}-registry-.*"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-{{ .Release.Name }}-registry-.*"} * 100) < 10
          for: 5m
          labels:
            tier: platform
            component: registry
          annotations:
            summary: "Docker Registry High Disk Usage"
            description: "Docker Registry has less than 10% disk space available."

        - alert: ElasticsearchDiskUsage
          expr: (sum(kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-{{ .Release.Name }}-elasticsearch-data-.*"}) / sum(kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-{{ .Release.Name }}-elasticsearch-data-.*"}) * 100) < 10
          for: 5m
          labels:
            tier: platform
            component: elasticsearch
          annotations:
            summary: "Elasticsearch High Disk Usage"
            description: "Elasticsearch has less than 10% disk space available."

        - alert: IngessCertificateExpiration
          expr: avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time() < 604800
          for: 10s
          labels:
            tier: platform
            component: ingress
          annotations:
            summary: "TLS Certificate Expiring Soon"
            {{/* We want '{{ $labels.host }}' to be evaluted by prometheus, not helm, so we have to escape it once */ -}}
            description: {{ printf "%q" "The TLS Certificate for {{ $labels.host }} is expiring in less than a week." }}

        - alert: PrometheusNotConnectedToAlertmanagers
          expr: prometheus_notifications_alertmanagers_discovered{job="prometheus"} < 1
          for: 10m
          labels:
            tier: platform
            component: prometheus
          annotations:
            summary: "Prometheus is not connected to any Alertmanagers"
            description: "Prometheus is not connected to any Alertmanagers"

        - alert: NginxIngressHighFailureRate
          expr: |
            (sum by (ingress) (rate(nginx_ingress_controller_requests{ingress!="", status=~"[4-5][0-9][0-9]"}[2m]))
              /
            sum by (ingress) (rate(nginx_ingress_controller_requests{ingress!=""}[2m]))) * 100 > 10
          for: 10m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: {{ printf "%q" "NGINX Ingress {{ $labels.ingress }} failure rate is {{ printf \"%.2f\" $value }}%." }}
            description: "This alert fires if the failure rate (the rate of 4xx or 5xx reponses)
              measured on a time window of 2 minutes was higher than 10% in the last
              10 minutes."

        - alert: KubeNamespaceStuckAtTerminating
          expr: |
            kube_namespace_status_phase{namespace=~".*-.*-.*-[0-9]{4}", phase="Terminating"} == 1
          for: 1h
          labels:
            tier: platform
            component: namespace
          annotations:
            summary: "Namespace in TERMINATING state for more than 1 hour"
            description: {{ printf "%q" "Namespace {{ $labels.namespace }} has been in a TERMINATING state for longer than an hour." }}

        - alert: ContainerMemoryAtTheLimit
          expr: |
            container_memory_working_set_bytes{container_name!="POD"} /
            container_spec_memory_limit_bytes{container_name!="POD"} > 0.8 < +Inf
          for: 1h
          labels:
            tier: platform
            component: container
          annotations:
            summary: "Container uses more than 80% of the memory limit"
            description: {{ printf "%q" "Memory usage for the {{ $labels.container }} Container in the {{ $labels.pod }} Pod is almost at the limit" }}

        - alert: PlatformContainerRestarting
          expr: |
            rate(kube_pod_container_status_restarts_total{namespace="astronomer"}[90s]) * 60 > 0
          labels:
            tier: platform
            severity: warning
          annotations:
            summary: "A container has restarted in the Astronomer platform namespace"
            description: {{ printf "%q" "The container {{ $labels.container }} in pod {{ $labels.pod }} has restarted at least once" }}

        - alert: PlatformContainerRestartingContinuously
          expr: |
            rate(kube_pod_container_status_restarts_total{namespace="astronomer"}[90s]) * 60 > 0
          for: 10m
          labels:
            tier: platform
            severity: urgent
          annotations:
            summary: "A container in the Astronomer platform namespace is stuck crash looping"
            description: {{ printf "%q" "The container {{ $labels.container }} in pod {{ $labels.pod }} is stuck crash looping" }}

        - alert: KubePodNotReady
          expr: |
            sum by (namespace, pod) (kube_pod_status_phase{job="kube-state", phase=~"Pending|Unknown"}) > 0
          for: 1h
          labels:
            tier: platform
            component: pod
          annotations:
            summary: "Pod in non-ready state for more than 1 hour"
            description: {{ printf "%q" "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than an hour." }}

        - alert: NginxIngressDown
          expr: |
            absent(up{job="nginx"} == 1)
          for: 15m
          labels:
            tier: platform
            component: nginx
          annotations:
            summary: "Nginx Ingress Controller has disappeared from Prometheus target discovery"
            description: "This alert fires if Prometheus target discovery was not able to
              reach ingress-nginx-metrics in the last 15 minutes."

        {{- if .Values.global.veleroEnabled }}
        - alert: VeleroMetricsAbsent
          expr: absent(up{job="velero"} == 1)
          for: 15m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero has disappeared from Prometheus target discovery"
            description: "This alert fires if Prometheus target discovery was not able to
              scrape Velero metrics in the last 15 minutes."

        - alert: FailedVeleroBackup
          expr: increase(velero_backup_failure_total{job="velero"}[1d]) > 0
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero failed to backup"
            description: {{ printf "%q" "Backup attempt failed in Velero past 1 day \n LABELS - {{ $labels }}." }}

        - alert: PartialFailedVeleroBackup
          expr: increase(velero_backup_partial_failure_total{job="velero"}[1d]) > 0
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "Velero partially failed to backup"
            description: {{ printf "%q" "Backup attempt partially failed in Velero past 1 day \n LABELS - {{ $labels }}." }}

        - alert: VeleroNoBackupForLong
          expr: time() - velero_backup_last_successful_timestamp > 172800
          for: 5m
          labels:
            tier: platform
            component: velero
          annotations:
            summary: "2 days since last successful Backup for a Velero Schedule"
            description: {{ printf "%q" "Time since last successful backup for {{ $labels.schedule }} :\n  VALUE = {{ $value }}\n  LABELS - {{ $labels }}" }}
        {{- end }}
